{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bec2426",
   "metadata": {},
   "source": [
    "# ðŸ“ž Call Quality Analyzer â€” Final (Colab-ready)\n",
    "\n",
    "**What it does (assignment-compliant):**\n",
    "- Works with the required test file.\n",
    "- Handles poor audio quality with simple normalization.\n",
    "- Uses **faster-whisper (tiny)** for very fast transcription on Colab.\n",
    "- Computes: **talk-time ratio, # questions, longest monologue, sentiment, one actionable insight**.\n",
    "- Attempts simple speaker identification (bonus).\n",
    "- Saves final results to `call_report.txt`.\n",
    "\n",
    "**How to run:** Open this notebook in Google Colab, set Runtime â†’ GPU (recommended), then Run â†’ Run all cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab). First run may take ~1-2 minutes.\n",
    "!pip install -q pytube faster-whisper transformers torch pydub librosa webrtcvad==2.0.10 soundfile\n",
    "!apt-get update -qq && apt-get install -y -qq ffmpeg\n",
    "print('Installed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & helpers\n",
    "import os, math, time, re, tempfile, shutil\n",
    "from pathlib import Path\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment, effects\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import pipeline\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Device:', device)\n",
    "\n",
    "# Helper: download audio using pytube (more stable in Colab)\n",
    "def download_audio_pytube(youtube_url, out_path='call_audio.mp3'):\n",
    "    print('Downloading with pytube...')\n",
    "    yt = YouTube(youtube_url)\n",
    "    stream = yt.streams.filter(only_audio=True).order_by('abr').desc().first()\n",
    "    stream.download(filename=out_path)\n",
    "    return out_path\n",
    "\n",
    "# Helper: convert to WAV mono 16k and normalize audio to help poor quality\n",
    "def to_wav_mono_16k(src, dst='call_audio.wav'):\n",
    "    audio = AudioSegment.from_file(src)\n",
    "    audio = effects.normalize(audio)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
    "    audio.export(dst, format='wav')\n",
    "    return dst\n",
    "\n",
    "print('Helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN: download -> preprocess -> transcribe\n",
    "YOUTUBE_URL = 'https://www.youtube.com/watch?v=4ostqJD3Psc'\n",
    "AUDIO_MP3 = 'call_audio.mp3'\n",
    "AUDIO_WAV = 'call_audio.wav'\n",
    "\n",
    "start_time = time.time()\n",
    "# Download (use pytube); if it fails, user can upload manually\n",
    "try:\n",
    "    if not os.path.exists(AUDIO_MP3):\n",
    "        download_audio_pytube(YOUTUBE_URL, out_path=AUDIO_MP3)\n",
    "    else:\n",
    "        print('Audio already downloaded')\n",
    "except Exception as e:\n",
    "    print('Download failed:', e)\n",
    "    raise\n",
    "\n",
    "# Convert & normalize\n",
    "print('Converting to WAV (mono, 16k)...')\n",
    "to_wav_mono_16k(AUDIO_MP3, dst=AUDIO_WAV)\n",
    "print('WAV ready:', AUDIO_WAV)\n",
    "\n",
    "# Load Whisper tiny model (fast)\n",
    "print('Loading Whisper (tiny) model...')\n",
    "model = WhisperModel('tiny', device=device)\n",
    "print('Transcribing... (this should be fast)')\n",
    "segments, info = model.transcribe(AUDIO_WAV, beam_size=5)\n",
    "\n",
    "transcript = []\n",
    "for seg in segments:\n",
    "    transcript.append({'start': round(seg.start,3), 'end': round(seg.end,3), 'text': seg.text.strip(), 'dur': round(seg.end-seg.start,3)})\n",
    "\n",
    "print(f'Transcribed {len(transcript)} segments in {round(time.time()-start_time,1)}s')\n",
    "for t in transcript[:5]:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d49d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS: talk-time, questions, longest monologue, sentiment, insight\n",
    "speaker_durations = {'Speaker_1':0.0, 'Speaker_2':0.0}\n",
    "speaker_segments = {'Speaker_1':[], 'Speaker_2':[]}\n",
    "for i, s in enumerate(transcript):\n",
    "    sp = 'Speaker_1' if i%2==0 else 'Speaker_2'\n",
    "    speaker_durations[sp] += s['dur']\n",
    "    speaker_segments[sp].append(s)\n",
    "\n",
    "total_speech_time = sum(speaker_durations.values()) if sum(speaker_durations.values())>0 else 1.0\n",
    "talk_ratio = {k: round(100*v/total_speech_time,1) for k,v in speaker_durations.items()}\n",
    "\n",
    "# Questions: count '?' and WH-patterns as fallback\n",
    "qcount = 0\n",
    "for s in transcript:\n",
    "    text = s['text']\n",
    "    if '?' in text:\n",
    "        qcount += text.count('?')\n",
    "    else:\n",
    "        if re.search(r\"\\b(what|why|how|when|where|who|is|are|do|did|can|could|would|should)\\b\", text.lower()):\n",
    "            qcount += 1\n",
    "\n",
    "# Longest monologue: max duration\n",
    "long_A = max([seg['dur'] for seg in speaker_segments['Speaker_1']] or [0])\n",
    "long_B = max([seg['dur'] for seg in speaker_segments['Speaker_2']] or [0])\n",
    "longest = max(long_A, long_B)\n",
    "longest_speaker = 'Speaker_1' if long_A>=long_B else 'Speaker_2'\n",
    "\n",
    "# Sentiment: use transformers pipeline on joined transcript (safe length)\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "joined_text = ' '.join([s['text'] for s in transcript])[:1000]\n",
    "sent_res = classifier(joined_text)\n",
    "\n",
    "# Actionable insight (rule-based)\n",
    "insight = ''\n",
    "if talk_ratio['Speaker_1'] > 70 or talk_ratio['Speaker_2'] > 70:\n",
    "    dom = 'Speaker_1' if talk_ratio['Speaker_1']>talk_ratio['Speaker_2'] else 'Speaker_2'\n",
    "    insight = f\"{dom} dominates the call ({talk_ratio[dom]}%). Suggest: ask more open-ended questions and pause to listen.\"\n",
    "elif qcount < 3:\n",
    "    insight = 'Too few questions asked. Encourage asking more open-ended questions.'\n",
    "else:\n",
    "    insight = 'Balanced speaking. Maintain concise summaries and clarifying questions.'\n",
    "\n",
    "# Likely sales rep heuristic\n",
    "sales_rep = 'Speaker_1' if talk_ratio['Speaker_1']>talk_ratio['Speaker_2'] else 'Speaker_2'\n",
    "\n",
    "# Print summary\n",
    "print('\\n===== CALL QUALITY REPORT =====')\n",
    "print('Talk-time ratio (%):', talk_ratio)\n",
    "print('Number of questions (heuristic):', qcount)\n",
    "print('Longest monologue (s):', longest, 'by', longest_speaker)\n",
    "print('Call sentiment:', sent_res)\n",
    "print('Actionable insight:', insight)\n",
    "print('Likely Sales Rep:', sales_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc539298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final report to a text file\n",
    "report_lines = []\n",
    "report_lines.append('ðŸ“ž Call Quality Report')\n",
    "report_lines.append('-'*40)\n",
    "report_lines.append(f'Talk-time ratio: {talk_ratio}')\n",
    "report_lines.append(f'Number of questions asked: {qcount}')\n",
    "report_lines.append(f'Longest monologue duration: {longest:.1f} seconds (by {longest_speaker})')\n",
    "report_lines.append(f'Overall sentiment: {sent_res[0][\"label\"]} ({sent_res[0][\"score\"]:.2f})')\n",
    "report_lines.append(f'Actionable insight: {insight}')\n",
    "report_lines.append(f'Likely Sales Rep: {sales_rep}')\n",
    "\n",
    "with open('call_report.txt','w') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "print('\\nâœ… Report saved as call_report.txt')\n",
    "print('You can download it from the left Files panel in Colab or run:')\n",
    "print(\"from google.colab import files; files.download('call_report.txt')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea549f7",
   "metadata": {},
   "source": [
    "## Short explanation (<=200 words)\n",
    "\n",
    "This notebook builds a fast Call Quality Analyzer that downloads the test YouTube audio using `pytube`, normalizes and converts it to 16 kHz mono WAV, and transcribes quickly using **faster-whisper (tiny)** â€” chosen to meet Colab free-tier time limits. We extract timestamped segments, assign segments alternately to two speakers (lightweight diarization), and compute talk-time ratio, number of questions (heuristic via punctuation/WH-words), the longest monologue, and overall sentiment using HuggingFace transformers. A rule-based system generates one actionable insight and the report is saved as `call_report.txt`. This design prioritizes speed, reproducibility on Colab, and explainability; for production, replace heuristic diarization with `pyannote` and upgrade the ASR model for higher accuracy.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
